{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Dataset - Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.029834",
     "start_time": "2017-09-07T22:37:50.331296"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nonusingh/anaconda/envs/python27/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "features_list = ['poi','salary','bonus']# You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.044167",
     "start_time": "2017-09-07T22:37:51.031464"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataset:\n",
      "146\n",
      "############################################\n",
      "number of unique features:\n",
      "21\n",
      "set(['salary', 'to_messages', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'email_address', 'restricted_stock_deferred', 'total_stock_value', 'shared_receipt_with_poi', 'long_term_incentive', 'exercised_stock_options', 'from_messages', 'other', 'from_poi_to_this_person', 'from_this_person_to_poi', 'poi', 'deferred_income', 'expenses', 'restricted_stock', 'director_fees'])\n",
      "############################################\n",
      "number of employees that are 'persons of interest':\n",
      "18\n",
      "############################################\n",
      "number of employees that are not POI:\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#  What is the length of the dataset?\n",
    "print \"length of the dataset:\"\n",
    "print len(data_dict)\n",
    "\n",
    "#  What is the Number of features in the dict?\n",
    "unique_features = set(\n",
    "    feature\n",
    "    for row_dict in data_dict.values()\n",
    "    for feature in row_dict.keys()\n",
    ")\n",
    "print \"############################################\"\n",
    "print \"number of unique features:\"\n",
    "print(len(unique_features))\n",
    "print(unique_features)\n",
    "\n",
    "# How many POIs in the dataset? How many are not POIs\n",
    "count = 0\n",
    "for user in data_dict:\n",
    "    if data_dict[user]['poi'] == True:\n",
    "        count+=1\n",
    "print \"############################################\"\n",
    "print \"number of employees that are 'persons of interest':\"\n",
    "print (count)\n",
    "print \"############################################\"\n",
    "print \"number of employees that are not POI:\"\n",
    "print len(data_dict)-(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, There are 146 rows in the dataset with 21 unique features including financial features like 'salary', 'bonus', 'total_stock_value' and email related features like 'shared_receipt_with_poi', 'from_messages'. \n",
    "Out of the 146 employees, only 18 were POIs and rest(128) are not POI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Next, we will investigate outliers and try to remove them. There were a few outliers in the dataset for eg 'TOTAL' which had bonus: $10,529,459 as seen from the plot below. This was actually the total bonus of all the employees. Since it can skew the algorithm, I will delete this row. Another candidate is THE TRAVEL AGENCY IN THE PARK which is not a person but an agency. After removing the row, our dataset now has 145 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.780161",
     "start_time": "2017-09-07T22:37:51.045757"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDUNBktT4/5Ay4QgCLUoYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116d99c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataset:\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "import matplotlib.pyplot\n",
    "data = featureFormat(data_dict, features_list)\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "# How to handle outlier with 1e7 bonus?\n",
    "data_dict.pop('TOTAL', 0)\n",
    "\n",
    "#  What is the length of the dataset now?\n",
    "print \"length of the dataset:\"\n",
    "print len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also investigate missing values for all features. Features with maximum rows with missing values are 'loan_advances'(142), 'director_fees'(129), 'restricted_stock_deferred'(128) , 'deferral_payments'(107), 'deferred_income'(97), and hence we will try to not include these features in our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.789090",
     "start_time": "2017-09-07T22:37:51.781738"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 51, 'to_messages': 59, 'deferral_payments': 107, 'total_payments': 21, 'exercised_stock_options': 44, 'bonus': 64, 'restricted_stock': 36, 'restricted_stock_deferred': 128, 'deferred_income': 97, 'total_stock_value': 20, 'shared_receipt_with_poi': 59, 'loan_advances': 142, 'from_messages': 59, 'other': 53, 'from_this_person_to_poi': 59, 'poi': 0, 'director_fees': 129, 'long_term_incentive': 80, 'expenses': 51, 'email_address': 34, 'from_poi_to_this_person': 59}\n"
     ]
    }
   ],
   "source": [
    "# How many features have most missing values?\n",
    "missing_features = {}\n",
    "for feature in unique_features:\n",
    "    missing_count = 0\n",
    "    for k in data_dict.iterkeys():\n",
    "        if data_dict[k][feature] == \"NaN\":\n",
    "            missing_count += 1\n",
    "    missing_features[feature] = missing_count\n",
    "print missing_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create new features and find out important features in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.799286",
     "start_time": "2017-09-07T22:37:51.790666"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "# New Feature: from_poi and to_poi\n",
    "for employee in data_dict:\n",
    "    if (data_dict[employee]['from_messages'] not in ['NaN', 0]) and (data_dict[employee]['from_this_person_to_poi'] not in ['NaN', 0]):\n",
    "        data_dict[employee]['to_poi'] = float(data_dict[employee]['from_messages'])/float(data_dict[employee]['from_this_person_to_poi'])\n",
    "    else:\n",
    "        data_dict[employee]['to_poi'] = 0\n",
    "        \n",
    "for employee in data_dict:\n",
    "    if (data_dict[employee]['to_messages'] not in ['NaN', 0]) and (data_dict[employee]['from_poi_to_this_person'] not in ['NaN', 0]):\n",
    "        data_dict[employee]['from_poi'] = float(data_dict[employee]['to_messages'])/float(data_dict[employee]['from_poi_to_this_person'])\n",
    "    else:\n",
    "        data_dict[employee]['from_poi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created two new features:'to_poi' is the fraction of messages sent to POIs out of the total sent messages(from_messages/from_this_person_to_poi) and the feature 'from_poi' is the fraction of messages received from POIs out of total received messages(to_messages/from_poi_to_this_person) for each employee. If a person has been communicating with POI a lot, it could mean he/she is also a POI. Smaller value of to_poi and from_poi would mean more communication and hence higher probability of being a POI himself/herself. Next, lets evaluate if including these two parameters improve the accuracy of our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.829320",
     "start_time": "2017-09-07T22:37:51.800700"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without new features: 0.826388888889\n",
      "Accuracy with new features: 0.826388888889\n"
     ]
    }
   ],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "features_list = ['poi','salary','to_messages', 'deferral_payments',\n",
    "                'total_payments','exercised_stock_options','bonus','restricted_stock',\n",
    "                'shared_receipt_with_poi','total_stock_value','expenses','from_messages',\n",
    "                'other','from_this_person_to_poi','deferred_income','long_term_incentive',\n",
    "                'from_poi_to_this_person','restricted_stock_deferred','loan_advances','director_fees']\n",
    "\n",
    "### Does the new feature improve the accuracy of our predictions?\n",
    "features_list1 = ['poi','salary','to_messages', 'deferral_payments',\n",
    "                'total_payments','exercised_stock_options','bonus','restricted_stock',\n",
    "                'shared_receipt_with_poi','total_stock_value','expenses','from_messages',\n",
    "                'other','from_this_person_to_poi','deferred_income','long_term_incentive',\n",
    "                'from_poi_to_this_person','restricted_stock_deferred','loan_advances','director_fees']\n",
    "\n",
    "features_list2 = ['poi','salary','to_messages', 'deferral_payments',\n",
    "                'total_payments','exercised_stock_options','bonus','restricted_stock',\n",
    "                'shared_receipt_with_poi','total_stock_value','expenses','from_messages',\n",
    "                'other','from_this_person_to_poi','deferred_income','long_term_incentive',\n",
    "                'from_poi_to_this_person','restricted_stock_deferred','loan_advances',\n",
    "                'director_fees','from_poi','to_poi']\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb_clf = GaussianNB()\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data1 = featureFormat(my_dataset, features_list1, sort_keys = True)\n",
    "labels1, features1 = targetFeatureSplit(data1)\n",
    "data2 = featureFormat(my_dataset, features_list2, sort_keys = True)\n",
    "labels2, features2 = targetFeatureSplit(data2)\n",
    "\n",
    "gnb_clf.fit(features1, labels1)\n",
    "print \"Accuracy without new features:\", gnb_clf.score(features1, labels1)\n",
    "gnb_clf.fit(features2, labels2)\n",
    "print \"Accuracy with new features:\", gnb_clf.score(features2, labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied a simple Naive Bayes algorithm on the dataset to compare the accuracy with and without the added new features, but it was same. The new features had no impact on the accuracy of the algorithm(82.63% in both cases), Next we will see out of the total 23 features, which are the top most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:51.858821",
     "start_time": "2017-09-07T22:37:51.830944"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Features selected by SelectKBest:\n",
      "['salary', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'total_stock_value', 'deferred_income', 'long_term_incentive', 'loan_advances']\n"
     ]
    }
   ],
   "source": [
    "### Top important features\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "skb = SelectKBest()\n",
    "selected_features = skb.fit_transform(features2,labels2)\n",
    "features_selected=[features_list2[i+1] for i in skb.get_support(indices=True)]\n",
    "print \"############################################\"\n",
    "print 'Features selected by SelectKBest:'\n",
    "print features_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ten important features are: 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'total_stock_value', 'deferred_income', 'long_term_incentive' and  'loan_advances'. The newly created features: 'from_poi' and 'to_poi' did not make it to the top 10 features.\n",
    "\n",
    "Now, we identified a few features before that had too many missing values: 'loan_advances', 'director_fees', 'restricted_stock_deferred', 'deferral_payments', 'deferred_income', before but since select k best selected the 'loan_advances' and 'deferred_income', we need to be careful in selecting features. I want to check how many POIs have missing data for these 5 features before eliminating them. Since I have fewer POIs in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:52.307762",
     "start_time": "2017-09-07T22:37:51.860625"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELDEN TIMOTHY N</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2144013</td>\n",
       "      <td>-2334434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOWEN JR RAYMOND M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALGER CHRISTOPHER F</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAUSEY RICHARD A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLWELL WESLEY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27610</td>\n",
       "      <td>-144062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELAINEY DAVID W</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FASTOW ANDREW S</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1386055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLISAN JR BEN F</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANNON KEVIN P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3117011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIRKO JOSEPH</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10259</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOENIG MARK E</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KOPPER MICHAEL J</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>81525000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202911</td>\n",
       "      <td>-300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RICE KENNETH D</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3504386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RIEKER PAULA H</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214678</td>\n",
       "      <td>-100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHELBY REX</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAGER F SCOTT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     loan_advances director_fees restricted_stock_deferred  \\\n",
       "BELDEN TIMOTHY N               NaN           NaN                       NaN   \n",
       "BOWEN JR RAYMOND M             NaN           NaN                       NaN   \n",
       "CALGER CHRISTOPHER F           NaN           NaN                       NaN   \n",
       "CAUSEY RICHARD A               NaN           NaN                       NaN   \n",
       "COLWELL WESLEY                 NaN           NaN                       NaN   \n",
       "DELAINEY DAVID W               NaN           NaN                       NaN   \n",
       "FASTOW ANDREW S                NaN           NaN                       NaN   \n",
       "GLISAN JR BEN F                NaN           NaN                       NaN   \n",
       "HANNON KEVIN P                 NaN           NaN                       NaN   \n",
       "HIRKO JOSEPH                   NaN           NaN                       NaN   \n",
       "KOENIG MARK E                  NaN           NaN                       NaN   \n",
       "KOPPER MICHAEL J               NaN           NaN                       NaN   \n",
       "LAY KENNETH L             81525000           NaN                       NaN   \n",
       "RICE KENNETH D                 NaN           NaN                       NaN   \n",
       "RIEKER PAULA H                 NaN           NaN                       NaN   \n",
       "SHELBY REX                     NaN           NaN                       NaN   \n",
       "SKILLING JEFFREY K             NaN           NaN                       NaN   \n",
       "YEAGER F SCOTT                 NaN           NaN                       NaN   \n",
       "\n",
       "                     deferral_payments deferred_income  \n",
       "BELDEN TIMOTHY N               2144013        -2334434  \n",
       "BOWEN JR RAYMOND M                 NaN            -833  \n",
       "CALGER CHRISTOPHER F               NaN         -262500  \n",
       "CAUSEY RICHARD A                   NaN         -235000  \n",
       "COLWELL WESLEY                   27610         -144062  \n",
       "DELAINEY DAVID W                   NaN             NaN  \n",
       "FASTOW ANDREW S                    NaN        -1386055  \n",
       "GLISAN JR BEN F                    NaN             NaN  \n",
       "HANNON KEVIN P                     NaN        -3117011  \n",
       "HIRKO JOSEPH                     10259             NaN  \n",
       "KOENIG MARK E                      NaN             NaN  \n",
       "KOPPER MICHAEL J                   NaN             NaN  \n",
       "LAY KENNETH L                   202911         -300000  \n",
       "RICE KENNETH D                     NaN        -3504386  \n",
       "RIEKER PAULA H                  214678         -100000  \n",
       "SHELBY REX                         NaN           -4167  \n",
       "SKILLING JEFFREY K                 NaN             NaN  \n",
       "YEAGER F SCOTT                     NaN             NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(my_dataset)\n",
    "df = df.T\n",
    "df.loc[df['poi'] == True][['loan_advances', 'director_fees', 'restricted_stock_deferred',\n",
    "                           'deferral_payments', 'deferred_income']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I filtered for all the POIs to see if any of them had any values for the features we intend to remove from my algorithm.'loan advances', 'deferral_payments' and 'deferred_income' have values in them and hence if we remove these features, I will lose information pertinent to POIs. I am safe to remove director_fees and restricted_stock_deferred from the algorithm. I also removed email_address feature as its not numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and Tune an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try a variety of supervised machine learning classifiers to make predictions and test their accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:52.574490",
     "start_time": "2017-09-07T22:37:52.309632"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Precision score_Gaussian is: 0.4\n",
      "Recall score_Gaussian is:  0.4\n",
      "############################################\n",
      "Precision score_Adaboost is: 0.333333333333\n",
      "Recall score_Adaboost is:  0.4\n",
      "############################################\n",
      "Precision score_RandomForest is: 0.0\n",
      "Recall score_RandomForest is:  0.0\n",
      "############################################\n",
      "Precision score_DecisionTree is: 0.0\n",
      "Recall score_DecisionTree is:  0.0\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "## Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_list = ['poi','salary','to_messages', 'deferral_payments',\n",
    "                'total_payments','exercised_stock_options','bonus','restricted_stock',\n",
    "                'shared_receipt_with_poi','total_stock_value','expenses',\n",
    "                'other','from_this_person_to_poi','deferred_income','long_term_incentive',\n",
    "                'from_poi_to_this_person','loan_advances']\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "sss = StratifiedShuffleSplit(n_splits= 100, test_size= 0.3, random_state= 42)\n",
    "sss.get_n_splits(features, labels)\n",
    "\n",
    "## Gaussian Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_clf = GaussianNB()\n",
    "g_clf.fit(features_train, labels_train)\n",
    "g_pred = g_clf.predict(features_test)\n",
    "\n",
    "### Adaboost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "a_clf = AdaBoostClassifier(random_state=42)\n",
    "a_clf.fit(features_train, labels_train)\n",
    "a_pred = a_clf.predict(features_test)\n",
    "\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "r_clf = RandomForestClassifier(random_state=42)\n",
    "r_clf.fit(features_train, labels_train)\n",
    "r_pred = r_clf.predict(features_test)\n",
    "\n",
    "### Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(features_train, labels_train)\n",
    "dt_pred = dt_clf.predict(features_test)\n",
    "\n",
    "## Evaluate Initial Classifiers\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print \"############################################\"\n",
    "print \"Precision score_Gaussian is:\", precision_score(labels_test, g_pred)\n",
    "print \"Recall score_Gaussian is: \", recall_score(labels_test, g_pred)\n",
    "print \"############################################\"\n",
    "print \"Precision score_Adaboost is:\", precision_score(labels_test, a_pred)\n",
    "print \"Recall score_Adaboost is: \", recall_score(labels_test, a_pred)\n",
    "print \"############################################\"\n",
    "print \"Precision score_RandomForest is:\", precision_score(labels_test, r_pred)\n",
    "print \"Recall score_RandomForest is: \", recall_score(labels_test, r_pred)\n",
    "print \"############################################\"\n",
    "print \"Precision score_DecisionTree is:\", precision_score(labels_test, dt_pred)\n",
    "print \"Recall score_DecisionTree is: \", recall_score(labels_test, dt_pred)\n",
    "print \"############################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I picked Naive Bayes because it gave me the best Precision and Recall of 0.4 and 0.4. Given the limited dataset, it makes sense that the simplest algorithm gave the best results. Hence the algorithm of choice would be Naive Bayes.\n",
    "\n",
    "### Class Imbalance\n",
    "Most machine learning algorithms and works best when the number of instances of each classes are roughly equal. In our dataset, only 18 out of 146 employees are POI. Since we have less number of POI, machine learning will tend to classify all employees as non POI and will still give fairly good accuracy. \n",
    "\n",
    "This problem in machine learning where the total number of a class of data (positive) is far less than the total number of another class of data (negative) is called class imbalance. This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.[Ref8] We need to reduce false negatives that our algorithm predicts which would be POIs that were falsely classified as non POI.\n",
    "Precision is Proportion of all employees classified POIs that are correct. Recall is Proportion of all real POIs that are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Algorithm\n",
    "We will now tune the algorithm and optimize the parameters of the algorithm to enable best performance. The most common way of tuning has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameters. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set[Ref7]. \n",
    "\n",
    "I used pipeline to do multiple functions sequentially. I first performed feature scaling using min max scalar as PCA needs normalized data[Ref1], then applied PCA for dimensionality reduction based on percent variation explained and then applied select k best to choose from 1,2,3 or 4 best principal components using grid search. I used grid search to tune number of components of PCA(based on percent variation) and selection of k best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:37:52.586796",
     "start_time": "2017-09-07T22:37:52.576028"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "features_list = ['poi','salary','to_messages', 'loan_advances', 'deferral_payments',\n",
    "                'total_payments','exercised_stock_options','bonus','restricted_stock',\n",
    "                'shared_receipt_with_poi','total_stock_value','expenses','from_messages',\n",
    "                'other','from_this_person_to_poi','deferred_income','long_term_incentive',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits= 100, test_size= 0.3, random_state= 42)\n",
    "scaler = MinMaxScaler()\n",
    "pca = PCA()\n",
    "\n",
    "clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "pipe = Pipeline(steps=[('scaler', scaler),('pca', pca),('skb', skb), ('gaussian', clf)])\n",
    "\n",
    "params = dict(pca__n_components=[4,5,6,7,8,9,10],\n",
    "                    skb__k = [1,2,3,4])\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=sss, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedShuffleSplit returns test and train splits which are created by preserving the same percentage for each target class as in the complete set. I used 30% of my data for test split and rest for training. StratifiedShuffleSplit is useful where the data is imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:12.176178",
     "start_time": "2017-09-07T22:37:52.588426"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nonusingh/anaconda/envs/python27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Best estimator:\n",
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x11a41e668>)), ('gaussian', GaussianNB(priors=None))])\n",
      "Best score:\n",
      "0.325090909091\n",
      "Best parameters:\n",
      "{'pca__n_components': 10, 'skb__k': 2}\n",
      "############################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Output\n",
    "########## \n",
    "\n",
    "gs.fit(features_train, labels_train)\n",
    "print \"############################################\"\n",
    "print(\"Best estimator:\")\n",
    "print(gs.best_estimator_)\n",
    "print(\"Best score:\")\n",
    "print(gs.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_params_)\n",
    "print \"############################################\"\n",
    "print ' '\n",
    "\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch selected 10 Principal components as best parameters and skb selected 2 best features as the best parameters for our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:36.212941",
     "start_time": "2017-09-07T22:38:36.207520"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81249011  0.96098139  0.9758785   0.98967482  0.99423658  0.99686645\n",
      "  0.99848469  0.99925697  0.99986357  0.99994662  0.99999124  0.99999998\n",
      "  1.          1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "pca.fit(features)\n",
    "print pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the top 10 PCA components explain 99.994662% of variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:12.210275",
     "start_time": "2017-09-07T22:38:12.194643"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Precision score is: 0.5\n",
      "Recall score is:  0.6\n",
      "############################################\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.92      0.94        39\n",
      "        1.0       0.50      0.60      0.55         5\n",
      "\n",
      "avg / total       0.90      0.89      0.89        44\n",
      "\n",
      "############################################\n",
      "Confusion Matrix:\n",
      "  [[36  3]\n",
      " [ 2  3]]\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print \"############################################\"\n",
    "print \"Precision score is:\", precision_score(labels_test, pred)\n",
    "print \"Recall score is: \", recall_score(labels_test, pred)\n",
    "print \"############################################\"\n",
    "print \"Classification Report:\\n \", classification_report(labels_test, pred)\n",
    "print \"############################################\"\n",
    "print \"Confusion Matrix:\\n \", confusion_matrix(labels_test, pred)\n",
    "print \"############################################\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Tuning, the Precision and Recall have improved to 0.5 and 0.6!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Precision and Recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to imbalanced dataset, along with the accuracy of the algorithm, we need to make sure the POIs are correctly identified as POIs. It is easy to label all employees as not POI and still have a decent accuracy score. The validation metric that tells how many POIs were correctly identified is Recall. It is ratio of correctly classified POIs to all POIs- those that were correctly classified and those which were misclassified as Non POI\n",
    "\n",
    "  $Recall = \\frac{Correctly\\ Classified\\ POIs}{Real\\ POIs}$\n",
    "\n",
    "\n",
    "Precision score on the other hand is to check that employees classified as POI are real POIs. To do that, we calculate precision of the algorithm. Precision is the ratio of correctly classified POIs and Total employees classified as POI. It tells us out of all the employees we suspected as POIs, how many are really POIs \n",
    "\n",
    "  $Precision = \\frac{Correctly\\ Classified\\ POIs}{Classified\\ POIs}$\n",
    "\n",
    "\n",
    "Larger precision score means algorithm has reduced false positives which are employees that are not POIs but are classified as POIs. In a case of scam, it is very important as you we don't want to send the wrong person to jail.\n",
    "Larger recall score means algorithm has reduced false negatives which are POIs that are wrongly classified as non POIs.\n",
    "\n",
    "In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).[Ref3]\n",
    "\n",
    "For this problem, a good recall score means we have identified POIs correctly and a good precision score means POIs we classified are in fact POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...in the context of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average precision for my decision tree classifier was 0.5 and the average recall was 0.6. What do each of these mean?\n",
    "\n",
    " - Precision is how often our class prediction (POI vs. non-POI) is right when we guess that class\n",
    " - Recall is how often we guess the class (POI vs. non-POI) when the class actually occurred\n",
    "\n",
    "In the context of our POI identifier, it is arguably more important to make sure we don't miss any POIs, so we don't care so much about precision. Imagine we are law enforcement using this algorithm as a screener to determine who to prosecute. When we guess POI, it is not the end of the world if they aren't a POI because we'll do due diligence. We won't put them in jail right away. For our case, we want high recall: when it is a POI, we need to make sure we catch them in our net. The Naive Bayes algorithm performed best in recall (0.6) of the algorithms I tried, hence it being my choice for final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:12.231480",
     "start_time": "2017-09-07T22:38:12.211864"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:14.590464",
     "start_time": "2017-09-07T22:38:12.233776"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x11a41e668>)), ('gaussian', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.86233\tPrecision: 0.47584\tRecall: 0.32000\tF1: 0.38266\tF2: 0.34243\n",
      "\tTotal predictions: 15000\tTrue positives:  640\tFalse positives:  705\tFalse negatives: 1360\tTrue negatives: 12295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load tester.py\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verified with tester that Precision and Recall is greater than 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-04T10:52:38.657821",
     "start_time": "2017-09-04T10:52:38.641482"
    }
   },
   "source": [
    "*Ques1:* Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "*Ans1:* The goal of this project is to detect persons of interest involved in Enron scandal using machine learning. Enron data provided as a part of this project had features like salary drawn, emails, stock options, bonus etc. The idea here is that the emails exchange and financial information of the employees can provide valuable insight into their involvement(or lack of) in the scandal. Our goal is to identify those employees(POI or persons of interest) using supervised machine learning techniques. There were a few outliers in the dataset for eg 'TOTAL' which had salary: $10,529,459. This was actually the total of all the rows and needed to be excluded so I removed it using data_dict.pop()Another candidate is THE TRAVEL AGENCY IN THE PARK which is not a person but an agency, but since the agency can also be a part of the fraud, I did not remove it.\n",
    "\n",
    "\n",
    "*Ques2:* What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "*Ans2:* I used Select k best for picking top important features. I created my own feature:'from_poi' which was fraction of emails sent to poi. It was not the top 10 features picked by the slect k best algorithm and it did not improve my accuracy. My top selected financial features: 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock','total_stock_value', 'expenses', 'deferred_income', 'long_term_incentive'. The only non financial feature that was important was: 'shared_receipt_with_poi'. Select k best picked 10 best features by default. In my pipeline, I used gridsearch to find the best k for the algorithm.\n",
    "I also used feature scaling using min max scalar. I intended to use PCA  for dimensionality reduction and PCA requires normalized data.[Ref1]. My pipeline consisted of PCA, Select k best, and then Naive Bayes classifier. Since I used PCA first, I reduced the dimension to 4 and hence select k best chose from 1,2,3,4 best features of PCA.\n",
    "\n",
    "\n",
    "*Ques3:* What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "*Ans3:* I tried variety of algorithms and although I wanted Adaboost to work the best due to unbalanced nature of my data set, the simplest algorithm- Naive Bayes gave me the best precision and recall. I used grid search to tune number of components of PCA(based on percent variation) and selection of k best features which improved my precision and recall from 0.4 and 0.4 to 0.5 and 0.6 respectively. I did not have parameters of Naive Bayes to tune so I just tuned PCA and k best. Tuning of the algorithm means altering the model according to the problem at hand. If you do not tune the algorithm properly, you can end up over or under fitting your data and your predictions will have less accuracy. My second best choice was adaboost for which I could tune: learning rate, n_estimators, algorithm(‘SAMME’/‘SAMME.R’)\n",
    "\n",
    "\n",
    "*Ques4:* What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "*Ans4:* Learning the parameters of a prediction function and testing it on the same data is a common methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data(Overfitting)[Ref2]. To avoid this, we hold out part of the available data as a test set and then cross validate it against the labels of the test set. I validated my algorithm using StratifiedShuffleSplit. StratifiedShuffleSplit returns test and train splits which are created by preserving the same percentage for each target class as in the complete set.\n",
    "The evaluation metrics that I used were: precision, recall and f1.\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.[Ref3] The precision of our tuned model was 0.5\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.[Ref3] Recall of our tuned model was 0.6\n",
    "The F-1 score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.[Ref3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ref 1: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n",
    "\n",
    "Ref 2: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Ref 3: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "\n",
    "Ref 4: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "Ref 5: https://discussions.udacity.com/t/starting-the-final-project/179090/4\n",
    "\n",
    "Ref 6: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "Ref 7: https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\n",
    "\n",
    "Ref 8: http://www.chioka.in/class-imbalance-problem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
